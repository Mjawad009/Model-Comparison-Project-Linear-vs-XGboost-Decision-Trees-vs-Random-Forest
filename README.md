# ğŸ¡ Random Forests vs XGBoost: California Housing Price Prediction

This project compares the performance of two popular ensemble machine learning algorithms â€” **Random Forest Regressor** and **XGBoost Regressor** â€” on the **California Housing dataset**. The goal is to evaluate accuracy, training time, and generalization ability for predicting median house prices.

---

## ğŸ“Œ Project Objectives

- Load and explore the California Housing dataset
- Train two regression models:
  - `RandomForestRegressor`
  - `XGBRegressor`
- Compare:
  - Model accuracy (RÂ² score, RMSE)
  - Training times
  - Overfitting/underfitting behavior
    
  ## ğŸš€ Model Performance Comparison

| Model              | ğŸ§  Train Time | âš¡ Predict Time | ğŸ“‰ MSE    | ğŸ“ˆ RÂ² Score |
|--------------------|---------------|----------------|-----------|-------------|
| Linear Regression  | 0.007 s       | 0.004 s        | 0.5559    | 0.5758      |
| Decision Tree      | 0.301 s       | 0.003 s        | 0.4952    | 0.6221      |
| Random Forest      | 18.753 s      | 0.161 s        | 0.2554    | 0.8051      |
| XGBoost            | 0.426 s       | 0.009 s        | 0.2226    | 0.8301      |

---

### ğŸ” Insights

- âœ… **XGBoost** leads in accuracy, achieving the **highest RÂ² score** and **lowest MSE**.
- ğŸŒ² **Random Forest** performs very well in accuracy but takes significantly longer to train.
- ğŸ§® **Linear Regression** and **Decision Tree** are fast and simple, but less accurate.
- âš¡ **XGBoost** hits the **sweet spot** between speed and accuracyâ€”making it a **top choice for real-world applications**.

---

## ğŸ§  What Youâ€™ll Learn

- Core concepts of **Bias vs. Variance**
- How **ensemble methods** improve performance
- Practical difference between:
  - Bagging (`RandomForest`)
  - Boosting (`XGBoost`)
- Model evaluation using real-world data

---

## ğŸ“Š Dataset

- **Source**: California Housing dataset (`sklearn.datasets.fetch_california_housing`)
- **Target**: Median House Value (in $100,000s)
- **Features**: 8 numerical variables (e.g., Median Income, House Age, Rooms, Population)

---

## ğŸš€ Models Used

| Model                 | Ensemble Type | Method      | Strengths                     |
|----------------------|---------------|-------------|-------------------------------|
| Random Forest         | Bagging       | Parallel    | Reduces variance, robust      |
| XGBoost               | Boosting      | Sequential  | Reduces bias, high accuracy   |

---

## ğŸ› ï¸ Tech Stack

- Python 3.x
- Jupyter Notebook
- Scikit-learn
- XGBoost
- NumPy, Matplotlib

---

## ğŸ“ˆ Evaluation Metrics

- **RÂ² Score** â€“ Coefficient of determination
- **RMSE** â€“ Root Mean Squared Error
- **Training Time** â€“ Time taken to fit each model

---

## ğŸ§ª How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/random-forest-vs-xgboost.git
   cd random-forest-vs-xgboost
